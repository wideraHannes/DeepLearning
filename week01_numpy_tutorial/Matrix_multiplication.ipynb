{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wz1Y8RJylg1W"
   },
   "source": [
    "# From Numpy to PyTorch\n",
    "\n",
    "Numpy provides comprehensive funtionality around mathematical operations on arrays. It is hence a good starting point, but by no means tailored to Machine/Deep Learning. \n",
    "\n",
    "There are various big packages (also known as frameworks) that offer functionality specialized in Machine/Deep Learning.\n",
    "\n",
    "The two most well known ones are **PyTorch** from Meta and **TensorFlow** from Google. In this course, we will work with PyTorch as it provides a good tradeoff between ease of use for academic purposes and scalability to large projects in further reasearch. \n",
    "\n",
    "Here, we will examine one of the main features of using a framework such as PyTorch: utilization of the GPU for faster processing, and [Automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation).\n",
    "\n",
    "The core operation of Deep Learning is matrix multiplication. If we have a way to make this operation as fast as possible that would save us a lot of time. Let's test how much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by initializing two 3D tensors in numpy with shapes `64,768,1024` and `64,2048,512`. Now we want to multiply the last dimensions of these tensors. We will use the command `%timeit` for benchmarking. Note that we will need to switch the order of the dimensions.\n",
    "\n",
    "**Exercise**: Measure the runtime of the matrix multiplication $a @ b.T$ with the `%timeit` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bm4iK3B5lgXo",
    "outputId": "8f3b37d5-f827-47ec-e294-81a02a57ad65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280 ms ± 3.33 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Initialize the tensors in numpy. To make it fair we are using PyTorchs default precision, float32\n",
    "a = np.random.rand(10000,1024).astype(np.float32)\n",
    "b = np.random.rand(5000,1024).astype(np.float32)\n",
    "\n",
    "### START CODE HERE ### (≈ 1 line of code)\n",
    "# Perform matrix multiplication and store result to variable c\n",
    "c = np.matmul(a, b.T)\n",
    "# alternativly\n",
    "d = a @ b.T\n",
    "\n",
    "# check the resulting shape\n",
    "assert list(c.shape) == [10000,5000]\n",
    "\n",
    "# repeat the matrix multiplication with the %timeit in fornt of the command\n",
    "# note works only with ipython notebooks\n",
    "%timeit c = a @ b.T\n",
    "### END CODE HERE ###\n",
    "\n",
    "del a\n",
    "del b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: `~454 ms ± 19.7 ms per loop`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYPO7tMLuN85"
   },
   "source": [
    "# Matrix multiplication in pytorch\n",
    "\n",
    "Now we will execute the same matrix multiplication in pytorch. We will still use the CPU.\n",
    "\n",
    "**Exercise**: Measure the runtime of the same matrix multiplication in PyTorch with the `%timeit` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_2tc7nm-sxdZ",
    "outputId": "624b22d8-07ac-4dd5-adea-02c2bcd06a28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 5000]) torch.Size([10000, 5000]) tensor(0.)\n",
      "670 ms ± 11 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tensors in pytorch \n",
    "a = torch.rand(10000,1024)\n",
    "b = torch.rand(5000,1024)\n",
    "\n",
    "### START CODE HERE ### (≈ 1 line of code)\n",
    "# Perform matrix multiplication and store result to variable c\n",
    "c = torch.matmul(a,b.T)\n",
    "d = a@b.T\n",
    "\n",
    "assert list(c.shape) == [10000,5000]\n",
    "print(d.shape, c.shape, torch.sum(c-d))\n",
    "\n",
    "%timeit c = torch.matmul(a,b.T)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "`~703 ms ± 27.6 ms per loop `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suUVVPWlvoUD"
   },
   "source": [
    "# Matrix multiplication in the GPU with pytorch\n",
    "\n",
    "Now we repeat the same process in the GPU. You can move a tensor to the gpu by calling `a = a.cuda()` or `a = a.to(\"cuda\")`. Similarly, you can move a tensor back to the CPU with `a = a.cpu()`.\n",
    "\n",
    "The resulting tensor needs to be in the CPU, otherwise `%timeit` gets stuck, since nothing is returned to the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3R4gDVrrvnkk",
    "outputId": "8ffe707b-7342-4748-e2eb-211cd0e1e924"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.6 ms ± 584 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (≈ 3 line of code)\n",
    "a = a.cuda()\n",
    "b = b.cuda()\n",
    "%timeit c = (a @ b.T).cpu()\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: `~41.3 ms ± 195 µs per loop`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGX90KEmz5bl"
   },
   "source": [
    "# Final notes\n",
    "\n",
    "The results may slighlty vary based on the hardware. You should see a significant improvement on the speedup of the matrix multiplication. On google colab, I got a speedup of ~7,4 times.\n",
    "\n",
    "In practice this means that if I could train a deep learning model on the cpu with numpy in 1 week, with the GPU in pytorch I can train it in a single day.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
